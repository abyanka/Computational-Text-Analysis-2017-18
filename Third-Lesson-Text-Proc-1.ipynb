{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# homework from previous day - we can discuss about them together after class or in the meeting-hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1) how to write a for loop?\n",
    "\n",
    " 2) write a if statement\n",
    "\n",
    " 3) which python objects have we learnt so far?\n",
    "\n",
    " 4) write a list\n",
    "\n",
    " 5) write a dictionary\n",
    "\n",
    " 6) which libraries did we use so far?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open the new dataset\n",
    "\n",
    "import codecs\n",
    "\n",
    "# or \\r\\n\n",
    "\n",
    "dataset = codecs.open(\"dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "# if you want to create a similar dataset -> https://github.com/fedenanni/collecting-news-from-the-internet-archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "let's break down what we are doing here\n",
    "\n",
    "1) we define a variable called \"dataset\"\n",
    "\n",
    "2) we use a library - called \"codecs\" for open it\n",
    "\n",
    "3) we open the file\n",
    "\n",
    "4) we give the path to the file, we say that it's in \"read\" mode (not write mode), and the encoding\n",
    "\n",
    "5) we read it\n",
    "\n",
    "6) we remove empty lines at the end (with strip)\n",
    "\n",
    "7) we split it with the breakline - if the file is created on windows you should use \"\\r\\n\"  <-- as in \"return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now - what type of objects is \"dataset\" ?\n",
    "\n",
    "\n",
    "# what is the length of \"dataset\" ?\n",
    "\n",
    "print (len(dataset))\n",
    "\n",
    "\n",
    "# how do we print a sample of \"dataset\" ?\n",
    "\n",
    "print (dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how to count the number of topics?\n",
    "\n",
    "#first, we create a list\n",
    "\n",
    "topics = []\n",
    "\n",
    "# then ,we write a for-loop\n",
    "\n",
    "for line in dataset:\n",
    "    topic = line.split(\"\\t\")[2]\n",
    "    topics.append(topic)\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "print (Counter(topics).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a different way of doing the same thing\n",
    "\n",
    "topics = [line.split(\"\\t\")[2] for line in dataset]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print Counter(topics).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's start with some real NLP\n",
    "\n",
    "# let's focus on a specific article, for example\n",
    "\n",
    "article = dataset[50].split(\"\\t\")[3]\n",
    "print (article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk # --> documentation: http://www.nltk.org/\n",
    "\n",
    "# you will also need this\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (type(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we start by dividing the text into sentences\n",
    "sentences = nltk.sent_tokenize(article) # <-- documentation for this command: http://www.nltk.org/_modules/nltk/tokenize.html\n",
    "\n",
    "# for checking what you're getting back from a library, run these commands\n",
    "print (type(sentences))\n",
    "print (len(sentences))\n",
    "print (sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us consider a single sentence - how do we do that? ## use the 5th sentence\n",
    "\n",
    "sentence = sentences[4]\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's divide the sentence in tokens (aka single words)\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "print (tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lower-casing the sentence\n",
    "without_capital_letters = [word.lower() for word in tokenized_sentence]\n",
    "\n",
    "print (without_capital_letters)\n",
    "\n",
    "# homework: write a for-loop for doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# homework: download stopwords <- google it out\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# what is \"stop\" ?\n",
    "\n",
    "print (stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "without_stop_words = [word for word in without_capital_letters if word not in stop]\n",
    "\n",
    "print (without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "\n",
    "# homework: how do we exclude punctuation?, hint: use exclude, from the previous line\n",
    "\n",
    "\n",
    "\n",
    "# homework: remove numbers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
